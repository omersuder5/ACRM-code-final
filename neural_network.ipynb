{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import catboost as cb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import scipy.sparse as sp\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import shap\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping na:  (396030, 25)\n",
      "emp_length    18301\n",
      "revol_util      276\n",
      "mort_acc      37795\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('Lending_Club_modelling_data_part1.txt', delimiter='\\t')\n",
    "df2 = pd.read_csv('Lending_Club_modelling_data_part2.txt', delimiter='\\t', dtype={'column_name': str}, low_memory=False)\n",
    "df3 = pd.read_csv('Lending_Club_modelling_data_part3.txt', delimiter='\\t')\n",
    "df4 = pd.read_csv('Lending_Club_modelling_data_part4.txt', delimiter='\\t')\n",
    "\n",
    "#merged data\n",
    "df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "print(\"Before dropping na: \",df.shape)\n",
    "\n",
    "# Drop the title column because it is the same information as purpose\n",
    "df = df.drop('title', axis=1)\n",
    "\n",
    "# Fill missing values in 'address' column with the wrong address in mort_acc column\n",
    "df['address'] = df['address'].fillna( '76093 Nicole Parks\\r\\nEast Donaldfurt, IN 70466')\n",
    "\n",
    "# Drop application_type, pub_rec, address, initial_list_status, pub_rec_bankruptcies columns since they are useless after checking catboost feature importance\n",
    "df = df.drop(['application_type', 'pub_rec', 'address', 'initial_list_status', 'pub_rec_bankruptcies'], axis=1)\n",
    "\n",
    "# Replace missing values in 'emp_title' column with 'Unknown'\n",
    "df['emp_title'] = df['emp_title'].fillna('Unknown')\n",
    "\n",
    "#print rows with missing values, show only if different from 0\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "#replace target variable values with 0 and 1\n",
    "df['loan_status'] = df['loan_status'].replace({'Charged Off': 1, 'Fully Paid': 0})\n",
    "\n",
    "# Convert 'issue_d' column to datetime format\n",
    "df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%Y')\n",
    "\n",
    "# Replace values in 'emp_length' column with numerical values\n",
    "emp_length_map = {\n",
    "    '< 1 year': 0, '1 year': 1, '2 years': 2, '3 years': 3, '4 years': 4, '5 years': 5,\n",
    "    '6 years': 6, '7 years': 7, '8 years': 8, '9 years': 9, '10+ years': 10\n",
    "}\n",
    "df['emp_length'] = df['emp_length'].replace(emp_length_map)\n",
    "\n",
    "# Correct types of columns\n",
    "df['revol_util'] = df['revol_util'].replace({'f':'NaN'})\n",
    "df['revol_util'] = df['revol_util'].astype(float)\n",
    "df['total_acc'] = df['revol_util'].replace({'INDIVIDUAL':'NaN'})\n",
    "df['total_acc'] = df['total_acc'].astype(float)\n",
    "df['mort_acc'] = df['mort_acc'].replace({'76093 Nicole Parks\\r\\nEast Donaldfurt, IN 70466':'NaN'})\n",
    "df['mort_acc'] = df['mort_acc'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net Profit function to be used to compute the loss\n",
    "# Problem: All my values are scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_profit_for_custom_loss(y_pred, batch):\n",
    "    # Unpack the batch into input (X) and labels (y)\n",
    "    X, y = batch\n",
    "\n",
    "    # Convert X and y to float tensors (if necessary)\n",
    "    X = X.float()\n",
    "    y = y.float()\n",
    "\n",
    "    # Extract relevant columns from the input (X) tensor\n",
    "    loan_amnt = X[:, 0]   # Assuming the first column is loan amount\n",
    "    int_rate = X[:, 2]    # Assuming the third column is interest rate\n",
    "    loan_status = y       # Assuming y represents loan status (0 or 1)\n",
    "\n",
    "    # Who received loans with the model (assuming y_pred is a tensor)\n",
    "    index = (y_pred == 0)\n",
    "\n",
    "    # Who paid back the loans (loan_status == 0)\n",
    "    index_payback = (loan_status == 0)\n",
    "\n",
    "    # Who defaulted on the loans (loan_status == 1)\n",
    "    index_default = (loan_status == 1)\n",
    "\n",
    "    # Calculate profit for loans that were paid back\n",
    "    paid_back_loans = index & index_payback\n",
    "    profit_paid_back = loan_amnt[paid_back_loans] * (int_rate[paid_back_loans] / 100)\n",
    "    total_profit = torch.sum(profit_paid_back)\n",
    "\n",
    "    # Calculate loss for loans that defaulted\n",
    "    defaulted_loans = index & index_default\n",
    "    loss_defaulted = loan_amnt[defaulted_loans] * 0.5\n",
    "    total_loss = torch.sum(loss_defaulted)\n",
    "\n",
    "    # Calculate net profit (custom loss)\n",
    "    net_profit = total_profit - total_loss\n",
    "\n",
    "    return net_profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net Profit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the net_profit function \n",
    "def net_profit(y_pred, df):\n",
    "    # Extract relevant columns from DataFrame as NumPy arrays\n",
    "    loan_amnt = df['loan_amnt'].values\n",
    "    int_rate = df['int_rate'].values\n",
    "    loan_status = df['loan_status'].values\n",
    "    \n",
    "    # Who received loans with the model\n",
    "    index = (y_pred == 0)\n",
    "    # Who paid back the loans\n",
    "    index_payback = (loan_status == 0)\n",
    "    # Who defaulted on the loans\n",
    "    index_default = (loan_status == 1)\n",
    "    \n",
    "    # Calculate profit for loans that were paid back\n",
    "    paid_back_loans = index & index_payback\n",
    "    profit_paid_back = loan_amnt[paid_back_loans] * (int_rate[paid_back_loans] / 100)\n",
    "    total_profit = np.sum(profit_paid_back)\n",
    "    \n",
    "    # Calculate loss for loans that defaulted\n",
    "    defaulted_loans = index & index_default\n",
    "    loss_defaulted = loan_amnt[defaulted_loans] * 0.5\n",
    "    total_loss = np.sum(loss_defaulted)\n",
    "    \n",
    "    return total_profit - total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second net profit function with installments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the net_profit function \n",
    "def net_profit2(y_pred, df):\n",
    "    # Extract relevant columns from DataFrame as NumPy arrays\n",
    "    loan_amnt = df['loan_amnt'].values\n",
    "    installment = df['installment'].values\n",
    "    term = df['term'].replace({-1: 36, 1: 60}).values\n",
    "    loan_status = df['loan_status'].values\n",
    "    \n",
    "    # Who received loans with the model\n",
    "    index = (y_pred == 0)\n",
    "    # Who paid back the loans\n",
    "    index_payback = (loan_status == 0)\n",
    "    # Who defaulted on the loans\n",
    "    index_default = (loan_status == 1)\n",
    "    \n",
    "    # Calculate profit for loans that were paid back\n",
    "    paid_back_loans = index & index_payback\n",
    "    profit_paid_back = installment[paid_back_loans]*term[paid_back_loans]-loan_amnt[paid_back_loans]\n",
    "    total_profit = np.sum(profit_paid_back)\n",
    "    \n",
    "    # Calculate loss for loans that defaulted\n",
    "    defaulted_loans = index & index_default\n",
    "    loss_defaulted = loan_amnt[defaulted_loans] * 0.5 #paid back %50 of the loan before defalting (assumption)\n",
    "    total_loss = np.sum(loss_defaulted)\n",
    "    \n",
    "    return total_profit - total_loss\n",
    "    #return total_profit,total_loss,total_profit - total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss\n",
    "I dont use it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom loss function based on net_profit\n",
    "class CustomNetProfitLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNetProfitLoss, self).__init__()\n",
    "\n",
    "    def net_profit_for_custom_loss(self, y_pred, batch):\n",
    "        # Unpack the batch into input (X) and labels (y)\n",
    "        X, y = batch\n",
    "\n",
    "        # Convert X and y to float tensors (if necessary)\n",
    "        X = X.float()\n",
    "        y = y.float()\n",
    "\n",
    "        # Extract relevant columns from the input (X) tensor\n",
    "        loan_amnt = X[:, 0]   # Assuming the first column is loan amount\n",
    "        int_rate = X[:, 2]    # Assuming the third column is interest rate\n",
    "        loan_status = y       # Assuming y represents loan status (0 or 1)\n",
    "\n",
    "        # Who received loans with the model (assuming y_pred is a tensor)\n",
    "        index = (y_pred == 0)\n",
    "\n",
    "        # Who paid back the loans (loan_status == 0)\n",
    "        index_payback = (loan_status == 0)\n",
    "\n",
    "        # Who defaulted on the loans (loan_status == 1)\n",
    "        index_default = (loan_status == 1)\n",
    "\n",
    "        # Calculate profit for loans that were paid back\n",
    "        paid_back_loans = index & index_payback\n",
    "        profit_paid_back = loan_amnt[paid_back_loans] * (int_rate[paid_back_loans] / 100)\n",
    "        total_profit = torch.sum(profit_paid_back)\n",
    "\n",
    "        # Calculate loss for loans that defaulted\n",
    "        defaulted_loans = index & index_default\n",
    "        loss_defaulted = loan_amnt[defaulted_loans] * 0.5\n",
    "        total_loss = torch.sum(loss_defaulted)\n",
    "\n",
    "        # Calculate net profit (custom loss)\n",
    "        net_profit = total_profit - total_loss\n",
    "\n",
    "        return net_profit\n",
    "\n",
    "    def forward(self, y_pred, batch):\n",
    "        # Compute net profit based on predicted labels and loan information\n",
    "        net_profit = self.net_profit_for_custom_loss(y_pred, batch)\n",
    "        \n",
    "        # Return negative net profit as loss (minimize loss to maximize net profit)\n",
    "        return -net_profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR TERM (36-60 MONTHS) WE ENCODE THEM AS 0-1 AND INCLUDE THEM IN THE NUMERICAL COLUMNS\n",
    "df['term'] = df['term'].str.replace(' months', '').astype(int)\n",
    "df['term'] = df['term'].replace({ 36: -1, 60: 1})\n",
    "\n",
    "#For verification status we use one hot encoding\n",
    "df = pd.get_dummies(df, columns=['verification_status'], sparse=True)\n",
    "df['verification_status_Not Verified'] = df['verification_status_Not Verified'].astype(int)\n",
    "#df['verification_status_Source Verified'] = df['verification_status_Source Verified'].astype(int) \n",
    "df['verification_status_Verified'] = df['verification_status_Verified'].astype(int)\n",
    "df.drop('verification_status_Source Verified', axis=1, inplace=True) #drop this column as it is redundant (control)\n",
    "\n",
    "#For home ownership we use one hot encoding\n",
    "df = pd.get_dummies(df, columns=['home_ownership'], sparse=True)\n",
    "df['home_ownership_MORTGAGE'] = df['home_ownership_MORTGAGE'].astype(int)\n",
    "df['home_ownership_OWN'] = df['home_ownership_OWN'].astype(int)\n",
    "df['home_ownership_RENT'] = df['home_ownership_RENT'].astype(int)\n",
    "df.drop('home_ownership_ANY', axis=1, inplace=True) #drop this column for control\n",
    "df.drop('home_ownership_NONE', axis=1, inplace=True) #drop this column for control\n",
    "df.drop('home_ownership_OTHER', axis=1, inplace=True) #drop this column for control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a neural network model, we drop the categorical columns in the beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include='object').columns.tolist()\n",
    "df.drop(categorical_columns, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original=df.copy()\n",
    "# Separate the data into three groups based on the year ranges\n",
    "original_training_set = df_original[df_original['issue_d'].dt.year < 2015]\n",
    "original_validation_set = df_original[df_original['issue_d'].dt.year == 2015]\n",
    "original_test_set = df_original[df_original['issue_d'].dt.year == 2016]\n",
    "\n",
    "# Optionally, you may want to reset the index for each subset\n",
    "original_training_set.reset_index(drop=True, inplace=True)\n",
    "original_validation_set.reset_index(drop=True, inplace=True)\n",
    "original_test_set.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['annual_inc']=df['annual_inc']/12 #convert annual income to monthly income\n",
    "df['installment_over_income'] = df['installment'] / df['annual_inc']\n",
    "df['installment_over_income'] = df['installment_over_income'].replace([np.inf, -np.inf], np.nan)\n",
    "df.drop('installment', axis=1, inplace=True) #drop this column as it is redundant\n",
    "df.drop('int_rate', axis=1, inplace=True) #redundant\n",
    "df.drop('loan_amnt', axis=1, inplace=True)\n",
    "df.drop('annual_inc', axis=1, inplace=True) #drop this column as it is redundant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 273678\n",
      "Number of rows in validation set: 94264\n",
      "Number of rows in test set: 28088\n"
     ]
    }
   ],
   "source": [
    "# Convert 'issue_d' column to datetime format\n",
    "df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%Y')\n",
    "\n",
    "# Separate the data into three groups based on the year ranges\n",
    "training_set = df[df['issue_d'].dt.year < 2015]\n",
    "validation_set = df[df['issue_d'].dt.year == 2015]\n",
    "test_set = df[df['issue_d'].dt.year == 2016]\n",
    "\n",
    "# Optionally, you may want to reset the index for each subset\n",
    "training_set.reset_index(drop=True, inplace=True)\n",
    "validation_set.reset_index(drop=True, inplace=True)\n",
    "test_set.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the number of rows in each group\n",
    "print(\"Number of rows in training set:\", len(training_set))\n",
    "print(\"Number of rows in validation set:\", len(validation_set))\n",
    "print(\"Number of rows in test set:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['term', 'emp_length', 'loan_status', 'dti', 'open_acc', 'revol_bal', 'revol_util', 'total_acc', 'mort_acc', 'verification_status_Not Verified', 'verification_status_Verified', 'home_ownership_MORTGAGE', 'home_ownership_OWN', 'home_ownership_RENT', 'installment_over_income']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = training_set.select_dtypes(include='object').columns.tolist()\n",
    "print(categorical_columns)\n",
    "numerical_columns = training_set.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(numerical_columns)\n",
    "print(len(numerical_columns)+len(categorical_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imputer and scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the training set:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "recover_original_training_set=training_set.loc[:,numerical_columns].copy()\n",
    "num_training_set = training_set.loc[:,numerical_columns].drop('loan_status', axis=1)\n",
    "numerical_columns_without_loan_status= num_training_set.columns\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "imputer.fit(num_training_set)\n",
    "\n",
    "imputed_array = imputer.transform(num_training_set[numerical_columns_without_loan_status])\n",
    "num_training_set = pd.DataFrame(imputed_array, columns=numerical_columns_without_loan_status)\n",
    "\n",
    "# Initialize scalers\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the numerical features\n",
    "num_training_set = robust_scaler.fit_transform(num_training_set)\n",
    "\n",
    "# Concatenate imputed features with target variable\n",
    "training_set.loc[:,numerical_columns_without_loan_status] = num_training_set\n",
    "\n",
    "# Check for missing values in the training set\n",
    "print(\"Missing values in the training set:\")\n",
    "print(training_set.isnull().sum()[training_set.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing is over, now we have a data set with only numerical features wiht some categorical features one hot encoded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Initialize the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PD_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1=64):\n",
    "        super(PD_NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=F.relu(self.fc1(x)) #works much better than relu (here i can also put relu but second has to be tanh)\n",
    "        x=F.tanh(self.fc2(x))\n",
    "        x=F.sigmoid(x)  # Add sigmoid activation function for output as probability\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop issue date column from the training set because it is of type object and we cant process them \n",
    "training_set = training_set.drop('issue_d', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PD_NN(input_size=len(numerical_columns_without_loan_status),\n",
    "              hidden_size1=128 \n",
    "              )\n",
    "\n",
    "explainer = shap.DeepExplainer(model, torch.tensor(training_set.drop('loan_status',axis=1).values).float())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.1)\n",
    "\n",
    "batch_size = 128 #maybe 32 instead\n",
    "num_epochs = 10\n",
    "\n",
    "# Apply SMOTE only to the training data\n",
    "smote = SMOTE(sampling_strategy = 1 , random_state=42)\n",
    "X_smoted, y_smoted = smote.fit_resample(training_set.drop('loan_status', axis =1), training_set['loan_status'])\n",
    "\n",
    "# WITHOUT SMOTE\n",
    "#X_smoted,y_smoted = training_set.drop('loan_status', axis =1), training_set['loan_status']\n",
    "\n",
    "# Define the train_dataset \n",
    "train_dataset = TensorDataset(torch.Tensor(X_smoted.values), torch.LongTensor(y_smoted.values))\n",
    "\n",
    "# Create the train_loader and test_loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap_values = explainer.shap_values(torch.tensor(training_set.drop('loan_status',axis=1).values).float())\n",
    "# ran for 33 min without any result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1559b2cb0>"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define class weights\n",
    "weights = torch.tensor([1]) # we dont use this and instead only SMOTE\n",
    "\n",
    "# Assuming you're using BCELoss\n",
    "criterion = nn.BCELoss(weight = weights)  # Using BCEWithLogitsLoss for numerical stability\n",
    "\n",
    "\n",
    "criterion3 = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 0.6834253668785095\n",
      "Epoch 2, Loss 0.6844149231910706\n",
      "Epoch 3, Loss 0.6777689456939697\n",
      "Epoch 4, Loss 0.6856172680854797\n",
      "Epoch 5, Loss 0.6841753721237183\n",
      "Epoch 6, Loss 0.6774819493293762\n",
      "Epoch 7, Loss 0.6808236837387085\n",
      "Epoch 8, Loss 0.6832053661346436\n",
      "Epoch 9, Loss 0.6830116510391235\n",
      "Epoch 10, Loss 0.6819921731948853\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X,y=batch\n",
    "        X.float()\n",
    "        y = y.float()\n",
    "        y_pred = model(X).squeeze()\n",
    "        y_pred_binary = (y_pred > 0.5).float()\n",
    "        #----------------------------------------------- comment in if you want to use the original loss for the NN\n",
    "        # original loss for the NN\n",
    "        loss = criterion(y_pred, y )\n",
    "        #----------------------------------------------- comment in if you want to use the MSE loss for the profit\n",
    "        #----------------------------------------------- if you do so, need to stop the scaling and smote\n",
    "        # MSE loss of the profit vs maximum possible profit\n",
    "        # profit=net_profit_for_custom_loss(y_pred_binary, batch)\n",
    "        # max_profit=net_profit_for_custom_loss(y, batch)\n",
    "        # loss = criterion3(profit, max_profit)\n",
    "        # loss.requires_grad=True\n",
    "\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "True Negative Rate (non default as non default): 0.6393312601381931\n",
      "True Positive Rate (default as default): 0.6046286031042128\n",
      "Accuracy: 0.6329262856349432\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.64      0.74    223166\n",
      "           1       0.28      0.60      0.38     50512\n",
      "\n",
      "    accuracy                           0.63    273678\n",
      "   macro avg       0.58      0.62      0.56    273678\n",
      "weighted avg       0.77      0.63      0.67    273678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train = training_set.drop('loan_status', axis =1), training_set['loan_status']\n",
    "\n",
    "X_train=torch.tensor(X_train.values, dtype=torch.float32)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "with torch.no_grad():\n",
    "    y_prob_tensor = model(X_train)\n",
    "    y_prob_numpy = y_prob_tensor.numpy()\n",
    "\n",
    "# Convert probabilities to predicted class labels (0 or 1) based on a threshold (e.g., 0.5)\n",
    "y_pred_train = (y_prob_numpy > 0.5).astype(int).flatten()\n",
    "\n",
    "\n",
    "# Assuming 'y' and 'y_pred' are the ground truth labels and predicted labels, respectively\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_train, y_pred_train)\n",
    "\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print(\"Training set:\")\n",
    "true_negative_rate = TN / (TN + FP)\n",
    "true_positive_rate = TP / (TP + FN)\n",
    "print(\"True Negative Rate (non default as non default):\", true_negative_rate)\n",
    "print(\"True Positive Rate (default as default):\", true_positive_rate)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net profit: 83506689.22999999\n",
      "Net profit original: 33980007.84500003\n",
      "Improved profit: 49526681.38499996\n"
     ]
    }
   ],
   "source": [
    "net_val_profit_model=net_profit(y_pred_train,original_training_set)\n",
    "print(\"Net profit:\", net_val_profit_model)\n",
    "\n",
    "# Create vector of 0s (i.e. everyone receievd the loan in the training set) of size equal to the number of rows in the training set \n",
    "zeros=np.zeros(len(original_training_set))\n",
    "net_val_profit=net_profit(zeros,original_training_set)\n",
    "print(\"Net profit original:\", net_val_profit)\n",
    "\n",
    "improved_profit=net_val_profit_model-net_val_profit\n",
    "print(\"Improved profit:\", improved_profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the validation set:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "recover_original_validation_set = validation_set.copy()\n",
    "num_validation_set = validation_set.loc[:,numerical_columns].drop('loan_status', axis=1)\n",
    "numerical_columns_without_loan_status= num_validation_set.columns\n",
    "\n",
    "imputed_array = imputer.transform(num_validation_set[numerical_columns_without_loan_status])\n",
    "num_validation_set = pd.DataFrame(imputed_array, columns=numerical_columns_without_loan_status)\n",
    "\n",
    "# Initialize scalers\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the numerical features\n",
    "num_validation_set_scaled = robust_scaler.fit_transform(num_validation_set)\n",
    "\n",
    "# Concatenate imputed features with target variable\n",
    "validation_set.loc[:,numerical_columns_without_loan_status] = num_validation_set_scaled\n",
    "\n",
    "# Check for missing values in the training set\n",
    "print(\"Missing values in the validation set:\")\n",
    "print(validation_set.isnull().sum()[validation_set.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop issue date column from the validation set because it is of type object and we cant process them \n",
    "validation_set = validation_set.drop('issue_d', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set:\n",
      "True Negative Rate (non default as non default): 0.7179867497280727\n",
      "True Positive Rate (default as default): 0.5351680654368849\n",
      "Accuracy: 0.6724624458966307\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.72      0.77     70791\n",
      "           1       0.39      0.54      0.45     23473\n",
      "\n",
      "    accuracy                           0.67     94264\n",
      "   macro avg       0.60      0.63      0.61     94264\n",
      "weighted avg       0.71      0.67      0.69     94264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_validation,y_validation = validation_set.drop('loan_status', axis=1), validation_set['loan_status']\n",
    "\n",
    "X_validation=torch.tensor(X_validation.values, dtype=torch.float32)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "with torch.no_grad():\n",
    "    y_prob_tensor = model(X_validation)\n",
    "    y_prob_numpy = y_prob_tensor.numpy()\n",
    "\n",
    "# Convert probabilities to predicted class labels (0 or 1) based on a threshold (e.g., 0.5)\n",
    "y_pred_validation = (y_prob_numpy > 0.5).astype(int).flatten()\n",
    "\n",
    "\n",
    "# Assuming 'y' and 'y_pred' are the ground truth labels and predicted labels, respectively\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_validation, y_pred_validation)\n",
    "\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print(\"Validation set:\")\n",
    "true_negative_rate = TN / (TN + FP)\n",
    "true_positive_rate = TP / (TP + FN)\n",
    "print(\"True Negative Rate (non default as non default):\", true_negative_rate)\n",
    "print(\"True Positive Rate (default as default):\", true_positive_rate)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "accuracy = accuracy_score(y_validation, y_pred_validation)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_validation, y_pred_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net profit: 5912333.892499983\n",
      "Net profit original: -48272015.629999995\n",
      "Improved profit: 54184349.52249998\n"
     ]
    }
   ],
   "source": [
    "# net profit of the model\n",
    "net_val_profit_model=net_profit(y_pred_validation,original_validation_set)\n",
    "print(\"Net profit:\", net_val_profit_model)\n",
    "\n",
    "# Create vector of 0s (i.e. everyone receievd the loan in the training set) of size equal to the number of rows in the training set \n",
    "zeros=np.zeros(len(original_validation_set))\n",
    "net_val_profit=net_profit(zeros,original_validation_set)\n",
    "print(\"Net profit original:\", net_val_profit)\n",
    "\n",
    "improved_profit=net_val_profit_model-net_val_profit\n",
    "print(\"Improved profit:\", improved_profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the test set:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "recover_original_test_set = test_set.copy()\n",
    "num_test_set = test_set.loc[:,numerical_columns].drop('loan_status', axis=1)\n",
    "numerical_columns_without_loan_status= num_test_set.columns\n",
    "\n",
    "imputed_array = imputer.transform(num_test_set[numerical_columns_without_loan_status])\n",
    "num_test_set = pd.DataFrame(imputed_array, columns=numerical_columns_without_loan_status)\n",
    "\n",
    "# Initialize scalers\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the numerical features\n",
    "num_test_set_scaled = robust_scaler.fit_transform(num_test_set)\n",
    "\n",
    "# Concatenate imputed features with target variable\n",
    "test_set.loc[:,numerical_columns_without_loan_status] = num_test_set_scaled\n",
    "\n",
    "# Check for missing values in the training set\n",
    "print(\"Missing values in the test set:\")\n",
    "print(test_set.isnull().sum()[test_set.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop issue date column from the test set because it is of type object and we cant process them \n",
    "test_set = test_set.drop('issue_d', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "True Negative Rate (non default as non default): 0.6253688524590164\n",
      "True Positive Rate (default as default): 0.6119848156182213\n",
      "Accuracy: 0.6236115066932498\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.63      0.74     24400\n",
      "           1       0.20      0.61      0.30      3688\n",
      "\n",
      "    accuracy                           0.62     28088\n",
      "   macro avg       0.56      0.62      0.52     28088\n",
      "weighted avg       0.82      0.62      0.68     28088\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test,y_test = test_set.drop('loan_status', axis=1), test_set['loan_status']\n",
    "\n",
    "X_test=torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "with torch.no_grad():\n",
    "    y_prob_tensor = model(X_test)\n",
    "    y_prob_numpy = y_prob_tensor.numpy()\n",
    "\n",
    "# Convert probabilities to predicted class labels (0 or 1) based on a threshold (e.g., 0.5)\n",
    "y_pred_test = (y_prob_numpy > 0.5).astype(int).flatten()\n",
    "\n",
    "\n",
    "# Assuming 'y' and 'y_pred' are the ground truth labels and predicted labels, respectively\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "print(\"Test set:\")\n",
    "true_negative_rate = TN / (TN + FP)\n",
    "true_positive_rate = TP / (TP + FN)\n",
    "print(\"True Negative Rate (non default as non default):\", true_negative_rate)\n",
    "print(\"True Positive Rate (default as default):\", true_positive_rate)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "improved profits in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net profit: 11815355.427500002\n",
      "Net profit original: 20667631.3575\n",
      "Improved profit: -8852275.93\n"
     ]
    }
   ],
   "source": [
    "# net profit of the model\n",
    "net_val_profit_model=net_profit(y_pred_test,original_test_set)\n",
    "print(\"Net profit:\", net_val_profit_model)\n",
    "\n",
    "# Create vector of 0s (i.e. everyone receievd the loan in the training set) of size equal to the number of rows in the training set \n",
    "zeros=np.zeros(len(original_test_set))\n",
    "net_val_profit=net_profit(zeros,original_test_set)\n",
    "print(\"Net profit original:\", net_val_profit)\n",
    "\n",
    "improved_profit=net_val_profit_model-net_val_profit\n",
    "print(\"Improved profit:\", improved_profit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RA-position",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
